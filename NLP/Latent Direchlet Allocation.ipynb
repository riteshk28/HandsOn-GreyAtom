{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ritesh.kankonkar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "#from gensim import corpora, models\n",
    "#import gensim\n",
    "from pprint import pprint\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "nltk.download('stopwords')\n",
    "en_stop = set(stopwords.words('english'))\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "# create sample documents\n",
    "# We will use author data\n",
    "import nltk\n",
    "text=open(\"LDA_test.txt\").read()\n",
    "sents = nltk.sent_tokenize(text)\n",
    "\n",
    "# compile sample documents into a list\n",
    "doc_set = sents\n",
    "\n",
    "# list for tokenized documents in loop\n",
    "texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### texts\n",
      "[['thai', u'polit', u'countri', 'young', 'men'], ['cricket', u'religi', u'follow', 'india', u'sometim', 'even', 'religion', u'take', 'back', 'seat', u'come', u'unit', u'peopl', 'cricket'], ['cricket', u'nation', 'sport', u'play', 'two', u'team', 'eleven', u'player', 'score', u'run', u'point', u'run', 'two', u'set', 'three', 'small', 'wooden', u'post', u'call', u'wicket'], ['england', u'dismiss', 'record', 'low', 'score', 'new', 'zealand', 'home', 'team', 'took', u'cautiou', 'approach', 'led', 'captain', 'kane', 'williamson', u'post', '18th', 'test', u'centuri', '\\xe2', 'new', 'zealand', 'record', '\\xe2', '102'], ['cricket', 'india', u'enthusiast', u'becom', 'religion'], ['cricket', 'india', u'enthusiast', u'becom', 'religion'], ['cricket', 'india', u'enthusiast', u'becom', 'religion'], ['cricket', 'india', u'enthusiast', u'becom', 'religion'], ['cricket', 'india', u'enthusiast', u'becom', 'religion'], ['cricket', 'india', u'enthusiast', u'becom', 'religion'], ['cricket', 'india', u'enthusiast', u'becom', 'religion'], ['cricket', 'india', u'enthusiast', u'becom', 'religion'], ['cricket', 'india', u'enthusiast', u'becom', 'religion'], ['cricket', 'india', u'enthusiast', u'becom', 'religion'], ['cricket', 'india', u'enthusiast', u'becom', 'religion'], ['cricket', 'india', u'enthusiast', u'becom', 'religion'], ['cricket', 'india', u'enthusiast', u'becom', 'religion'], ['cricket', 'india', u'enthusiast', u'becom', 'religion'], ['cricket', 'india', u'enthusiast', u'becom', 'religion'], ['cricket', 'india', u'enthusiast', u'becom', 'religion'], ['cricket', 'india', u'enthusiast', u'becom', 'religion'], ['cricket', 'india', u'enthusiast', u'becom', 'religion'], ['cricket', 'india', u'enthusiast', u'becom', 'religion'], ['cricket', 'india', u'enthusiast', u'becom', 'religion'], ['cricket', 'india', u'enthusiast', u'becom', 'religion'], ['cricket', 'india', u'enthusiast', u'becom', 'religion']]\n",
      "\n",
      "##### The lines in texts\n",
      "['thai', u'polit', u'countri', 'young', 'men']\n",
      "['cricket', u'religi', u'follow', 'india', u'sometim', 'even', 'religion', u'take', 'back', 'seat', u'come', u'unit', u'peopl', 'cricket']\n",
      "['cricket', u'nation', 'sport', u'play', 'two', u'team', 'eleven', u'player', 'score', u'run', u'point', u'run', 'two', u'set', 'three', 'small', 'wooden', u'post', u'call', u'wicket']\n",
      "['england', u'dismiss', 'record', 'low', 'score', 'new', 'zealand', 'home', 'team', 'took', u'cautiou', 'approach', 'led', 'captain', 'kane', 'williamson', u'post', '18th', 'test', u'centuri', '\\xe2', 'new', 'zealand', 'record', '\\xe2', '102']\n",
      "['cricket', 'india', u'enthusiast', u'becom', 'religion']\n",
      "['cricket', 'india', u'enthusiast', u'becom', 'religion']\n",
      "['cricket', 'india', u'enthusiast', u'becom', 'religion']\n",
      "['cricket', 'india', u'enthusiast', u'becom', 'religion']\n",
      "['cricket', 'india', u'enthusiast', u'becom', 'religion']\n",
      "['cricket', 'india', u'enthusiast', u'becom', 'religion']\n",
      "['cricket', 'india', u'enthusiast', u'becom', 'religion']\n",
      "['cricket', 'india', u'enthusiast', u'becom', 'religion']\n",
      "['cricket', 'india', u'enthusiast', u'becom', 'religion']\n",
      "['cricket', 'india', u'enthusiast', u'becom', 'religion']\n",
      "['cricket', 'india', u'enthusiast', u'becom', 'religion']\n",
      "['cricket', 'india', u'enthusiast', u'becom', 'religion']\n",
      "['cricket', 'india', u'enthusiast', u'becom', 'religion']\n",
      "['cricket', 'india', u'enthusiast', u'becom', 'religion']\n",
      "['cricket', 'india', u'enthusiast', u'becom', 'religion']\n",
      "['cricket', 'india', u'enthusiast', u'becom', 'religion']\n",
      "['cricket', 'india', u'enthusiast', u'becom', 'religion']\n",
      "['cricket', 'india', u'enthusiast', u'becom', 'religion']\n",
      "['cricket', 'india', u'enthusiast', u'becom', 'religion']\n",
      "['cricket', 'india', u'enthusiast', u'becom', 'religion']\n",
      "['cricket', 'india', u'enthusiast', u'becom', 'religion']\n",
      "['cricket', 'india', u'enthusiast', u'becom', 'religion']\n"
     ]
    }
   ],
   "source": [
    "# loop through document list\n",
    "for i in doc_set:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    #stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)\n",
    "\n",
    "print(\"\\n##### texts\")\n",
    "print(texts)\n",
    "\n",
    "print(\"\\n##### The lines in texts\")\n",
    "for line in texts:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
